# Abstract-Tweet-Summariser
Pipeline used to train
![image](https://github.com/sarthakharne/Abstract-Tweet-Summariser/assets/75558601/4893525a-4f75-4510-b614-e4d385f9831b)

## Key Techniques Used:
### Using ChatGPT as a teacher
The tweet dataset made up of concatenated tweets (pertaining to a particular trending topic) and their summary as provided by ChatGPT. We use this summary later to train our model
![image](https://github.com/sarthakharne/Abstract-Tweet-Summariser/assets/75558601/2d6f8149-1617-49fe-b6ae-f3c205785426)

### Using Adversarial Discriminator
We use the summary from ChatGPT and the summary generated by our model (T5-Base) to train the discriminator and then later use the adversarial loss, along with the language modelling loss to train our model.
![image](https://github.com/sarthakharne/Abstract-Tweet-Summariser/assets/75558601/920c2c01-94fb-4030-9a54-c44b87982c9e)

### Using BERT Score to evaluate semantics
BERT Score is used along with ROUGE to give a better evaluation on semantics. ROUGE gave high values even after the model overfit, so BERT Score was instrumental in checking if the model was overfitting and also it gave better insights than ROUGE as it wasn't just dependent on the word co-occurence.

Using all these resulted in a much better summariser rather than just using T5 for the same.
![image](https://github.com/sarthakharne/Abstract-Tweet-Summariser/assets/75558601/6bdab0b5-9c2d-4dd3-9d7e-ee2ba5925e0a)

Original Dataset used for the tweets: [Cross-domain Semantic Parsing via Paraphrasing](https://aclanthology.org/D17-1127) (Su & Yan, EMNLP 2017)
Modified Dataset used to train: https://www.kaggle.com/datasets/sarthakharne/tweetsummemnlp2017
Kaggle Notebook: Link to Notebook: https://www.kaggle.com/code/sarthakharne/mandate4submission
